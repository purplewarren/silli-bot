#!/usr/bin/env python3
"""
Reasoner info handlers for Silli Bot
Provides visibility into reasoner model status and performance
"""

import aiohttp
import os
from aiogram import Router, F
from aiogram.types import Message
from aiogram.filters import Command
from loguru import logger
from .i18n import get_locale

router_reasoner_info = Router()

REASONER_BASE_URL = os.getenv('REASONER_BASE_URL', 'http://localhost:5001')

@router_reasoner_info.message(Command("reason_model"))
async def handle_reason_model_command(message: Message):
    """Show reasoner model status and configuration"""
    locale = get_locale(message.chat.id)
    
    try:
        # Get reasoner status
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{REASONER_BASE_URL}/status", timeout=5) as response:
                if response.status == 200:
                    status = await response.json()
                    
                    # Format response based on locale
                    if locale == "pt_br":
                        text = "üß† **Status do Modelo de IA**\n\n"
                        
                        # Check for low-fidelity mode
                        model_hint = status.get('model_hint', 'N/A')
                        model_used = status.get('last_model_used', 'Nenhum')
                        if model_used != model_hint and model_used != 'Nenhum':
                            text += "üî∂ **MODO DE BAIXA FIDELIDADE**\n\n"
                        
                        text += f"**Ativo**: {'‚úÖ Sim' if status.get('enabled') else '‚ùå N√£o'}\n"
                        text += f"**Modelo Sugerido**: `{model_hint}`\n"
                        text += f"**√öltimo Modelo Usado**: `{model_used}`\n"
                        text += f"**Permite Fallback**: {'‚úÖ Sim' if status.get('allow_fallback') else '‚ùå N√£o'}\n"
                        text += f"**Taxa de Cache**: {status.get('cache_hit_rate', 0) * 100:.1f}%\n"
                        text += f"**Endpoint**: `{status.get('endpoint_host', 'unknown')}`\n"
                        
                        if status.get('fallback_occurred'):
                            text += f"\n‚ö†Ô∏è **Fallback Ocorreu**: {status.get('fallback_reason', 'Motivo desconhecido')}"
                        
                        cache_stats = status.get('cache_stats', {})
                        if cache_stats.get('enabled'):
                            text += f"\n\nüìä **Estat√≠sticas do Cache**:\n"
                            text += f"‚Ä¢ Acertos: {cache_stats.get('hits', 0)}\n"
                            text += f"‚Ä¢ Perdas: {cache_stats.get('misses', 0)}\n"
                            text += f"‚Ä¢ Tamanho: {cache_stats.get('size', 0)}"
                    else:
                        text = "üß† **AI Model Status**\n\n"
                        
                        # Check for low-fidelity mode
                        model_hint = status.get('model_hint', 'N/A')
                        model_used = status.get('last_model_used', 'None')
                        if model_used != model_hint and model_used != 'None':
                            text += "üî∂ **LOW-FIDELITY MODE**\n\n"
                        
                        text += f"**Enabled**: {'‚úÖ Yes' if status.get('enabled') else '‚ùå No'}\n"
                        text += f"**Model Hint**: `{model_hint}`\n"
                        text += f"**Last Model Used**: `{model_used}`\n"
                        text += f"**Allow Fallback**: {'‚úÖ Yes' if status.get('allow_fallback') else '‚ùå No'}\n"
                        text += f"**Cache Hit Rate**: {status.get('cache_hit_rate', 0) * 100:.1f}%\n"
                        text += f"**Endpoint**: `{status.get('endpoint_host', 'unknown')}`\n"
                        
                        if status.get('fallback_occurred'):
                            text += f"\n‚ö†Ô∏è **Fallback Occurred**: {status.get('fallback_reason', 'Unknown reason')}"
                        
                        cache_stats = status.get('cache_stats', {})
                        if cache_stats.get('enabled'):
                            text += f"\n\nüìä **Cache Stats**:\n"
                            text += f"‚Ä¢ Hits: {cache_stats.get('hits', 0)}\n"
                            text += f"‚Ä¢ Misses: {cache_stats.get('misses', 0)}\n"
                            text += f"‚Ä¢ Size: {cache_stats.get('size', 0)}"
                
                else:
                    if locale == "pt_br":
                        text = f"‚ùå Erro ao obter status do modelo (HTTP {response.status})"
                    else:
                        text = f"‚ùå Failed to get model status (HTTP {response.status})"
                        
    except aiohttp.ClientTimeout:
        if locale == "pt_br":
            text = "‚è±Ô∏è Timeout ao conectar com o servi√ßo de IA"
        else:
            text = "‚è±Ô∏è Timeout connecting to AI service"
    except Exception as e:
        logger.error(f"Error getting reasoner status: {e}")
        if locale == "pt_br":
            text = f"‚ùå Erro ao obter status: {str(e)}"
        else:
            text = f"‚ùå Error getting status: {str(e)}"
    
    await message.reply(text, parse_mode="Markdown")

@router_reasoner_info.message(Command("reason_models"))
async def handle_reason_models_command(message: Message):
    """List available AI models"""
    locale = get_locale(message.chat.id)
    
    try:
        # Get available models
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{REASONER_BASE_URL}/models", timeout=5) as response:
                if response.status == 200:
                    data = await response.json()
                    models = data.get('models', [])
                    
                    if locale == "pt_br":
                        text = "ü§ñ **Modelos de IA Dispon√≠veis**\n\n"
                        if models:
                            for model in models:
                                name = model.get('name', 'Unknown')
                                size = model.get('size', 0)
                                size_gb = round(size / (1024**3), 1) if size else 0
                                text += f"‚Ä¢ `{name}` ({size_gb} GB)\n"
                        else:
                            text += "Nenhum modelo dispon√≠vel"
                    else:
                        text = "ü§ñ **Available AI Models**\n\n"
                        if models:
                            for model in models:
                                name = model.get('name', 'Unknown')
                                size = model.get('size', 0)
                                size_gb = round(size / (1024**3), 1) if size else 0
                                text += f"‚Ä¢ `{name}` ({size_gb} GB)\n"
                        else:
                            text += "No models available"
                
                else:
                    if locale == "pt_br":
                        text = f"‚ùå Erro ao listar modelos (HTTP {response.status})"
                    else:
                        text = f"‚ùå Failed to list models (HTTP {response.status})"
                        
    except aiohttp.ClientTimeout:
        if locale == "pt_br":
            text = "‚è±Ô∏è Timeout ao conectar com o servi√ßo de IA"
        else:
            text = "‚è±Ô∏è Timeout connecting to AI service"
    except Exception as e:
        logger.error(f"Error listing models: {e}")
        if locale == "pt_br":
            text = f"‚ùå Erro ao listar modelos: {str(e)}"
        else:
            text = f"‚ùå Error listing models: {str(e)}"
    
    await message.reply(text, parse_mode="Markdown")
